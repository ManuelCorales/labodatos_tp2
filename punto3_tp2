#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sat Mar  2 15:15:41 2024

@author: NAtali Biasoni
"""


from sklearn.tree import DecisionTreeClassifier,plot_tree
import pandas as pd
from inline_sql import sql
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, RandomizedSearchCV,  KFold, cross_val_score
import sklearn.metrics
#%%
carpeta= "/home/oem/Desktop/uni/labodatos/"
señas_df= pd.read_csv(carpeta+"sign_mnist_train.csv")

#%%
#CREO UN DATASET SOLO DE VOCALES#
#HAy 1126 datos para la letra a
#hay 957 datos para la letra e
#hay 1162 datos para la letra i
#hay 1196 datos para la letra o
#hay 1161 datos para la letra u
df_vocales= sql^"""
         Select * 
         FROM señas_df
         WHERE label = 0 OR label =4 OR label = 8
          OR label =14 OR label = 20
         """
print(df_vocales.head())     


#Separo los datos para el entrenamiento y el test 
X= df_vocales.drop(columns = ['label'])
Y= df_vocales['label']

X_train, X_test, y_train, y_test =  train_test_split(X, Y, test_size=0.15 , shuffle=True, stratify=  Y)
#%%
#busco el mejor arbol
hyper_params = {'criterion' : ["gini", "entropy"],
                   'max_depth' : [2,3,4,5,6] }

arbol = DecisionTreeClassifier()
clf = RandomizedSearchCV(arbol, hyper_params, random_state= 0, n_iter= 5)
#el mejor arbol tienen los hyperparametros max depth = 6 y criterio = entropy
#lo hago a mano por las dudas
def busco_mejor_arbol (c:list, md:list):
    best_score= 0
    for criterio in c:
        for depth in md:
            arbol = DecisionTreeClassifier(criterion = criterio, max_depth=depth)
            scores = cross_val_score(arbol, X_train, y_train, cv=5)
            prom_score= np.mean(scores)
            if prom_score > best_score:
                best_score= prom_score
                hyperparams:list = []
                hyperparams.append(criterio)
                hyperparams.append(depth)
    return best_score, hyperparams          

print(busco_mejor_arbol( ["gini", "entropy"], [2,3,4,5,6] ))

clf.fit(X_train, y_train)

clf.best_params_

clf.best_score_

#el mejor arbol tienen los hyperparametros max depth = 6 y criterio = entropy
clf.score(X_test, y_test)
arbolFinal= DecisionTreeClassifier(max_depth= 6, criterion= 'entropy')
arbolFinal.fit(X_train, y_train)
arbolFinal.score(X_test, y_test)
plot_tree(arbolFinal)

#%%
#matriz de confusion
y_real= y_test
y_predicho= clf.predict(X_test)
mc= sklearn.metrics.confusion_matrix(y_real, y_predicho)
clases= ["A","E","I","O","U"]
plt.figure(figsize=(8, 6))
sns.set(font_scale=1.2)  # Ajusta el tamaño de la fuente
sns.heatmap(mc, annot=True, fmt='d', cmap='PuRd', 
            xticklabels=clases, yticklabels=clases)  # Personaliza el cmap según tus preferencias
plt.title('Matriz de Confusión')
plt.xlabel('Clases Predictivas')
plt.ylabel('Clases Verdaderas')
plt.show()

exactitud = sklearn.metrics.accuracy_score(y_real, y_predicho)
precision = sklearn.metrics.precision_score(y_real, y_predicho, average=None)
recall = sklearn.metrics.recall_score(y_real, y_predicho, average=None)

print('exactitud del modelo:', exactitud)
print('Precisión por clase:', precision)
print('Exhaustividad por clase:', recall)

